# Service mesh

So far, we have gradually improved our toy web app, SplitDim, into a real cloud native microservice application. We moved resource state out from the app into a key-value store data layer, making our app effectively stateless, we added various resilience mechanism to make the communication between the app and the key-value store more robust, and we showed several ways to manage the app without having to modify and redeploy the any Go code. These improvements were greatly supported, and in large parts made possible, by the built-in cloud native features offered by Kubernetes; for instance, Kubernetes lets is specify config files in ConfigMaps and map these easily and declaratively into the app's startup parameters, allow the app to reach the key-value store via a DNS name eliminating the tight coupling that would be created by communicating over a fix IP address, etc. 

In this lab we show that the cloud native support provided by Kubernetes does not stop here: in fact, *many resilience, manageability and observability patterns are readily provided by service meshes* on top of Kubernetes, without us having to write any specific code (in fact, most of the time we must even *remove* cloud native support from our app in order to avoid interfering with the cloud native features implemented in the service mesh). We will demonstrate the use of service meshes using our SplitDim web app and we will also take this occasion to cover the last remaining cloud native pillar: *observability*.

![SplitDim logo, generated by logoai.com.](/99-labs/fig/splitdim-logo.png)

The below tasks let you familiarize yourself with one of the most popular service mesh distributions, Istio. The service mesh concept, however, is much broader than what we have room to cover here, so take this only as a gentle introduction of the basic workings of a service mesh; we encourage you to [make your own research](https://istio.io/latest/docs) to get more info.

## Table of Contents

1. [Preliminaries](#preliminaries)
2. [Command line parameters](#command-line-parameters)
3. [Configuration files](#configuration-files)

## Preliminaries

The service mesh concept emerged from the need to obtain better tools to more efficiently manage modern applications, typically architected as distributed collections of microservices that together perform some concrete business function. A service mesh is a dedicated infrastructure layer on top of Kubernetes that can transparently add capabilities like observability, traffic management, and security, without having to add them to your own code. It is the communication between microservices that makes a distributed application possible. Correspondingly, most of the service mesh features address some critical aspect of internal and external *network communications* in Kubernetes, like HTTP request routing, load balancing, failure recovery, rate limiting, access control, encryption, and end-to-end authentication.

Below we use Istio and to demonstrate some of the useful features provided by a modern service mesh. We will use SplitDim to 

At this point, you should have Istio installed in your cluster. If not, go back to the first lab and make sure Istio, the command line config tool called `istioctl`, and the Kubernetes Gateway API are [installed](https://github.com/l7mp/learning-go/blob/master/99-labs/01-setup/README.md#install-istio). Next, we customize Istio to unlock some useful features which are not available in the minimal profile we have used this far.

1. Make sure `istioctl` is available in your shell.

   ```shell
   export ISTIO_DIR=<path-to-istio-root-dir>
   export PATH=${ISTIO_DIR}/bin:${PATH}
   ```
   
   Check that `istioctl` works fine; if yes, you should see some version info dumped by `istioctl` to the console.
   
   ```shell
   istioctl version
   ```

2. Reconfigure Istio with the `demo` profile. This will enable Prometheus for metric collection, Jaeger for tracing, Kiali for monitoring the service mesh, etc.

   ```shell
   istioctl install --set profile=demo -y
   ```
   
   > **Note**
   >
   > It make take a while until all Istio components come up online. Meanwhile, some Istio features may not be available. Watch the pod statuses in the `istio-system` namespace to test whether everything works fine: `kubectl get pods -n istio-system -o wide`.

3. Enable tracing. In particular, the below configuration will instruct Istio to collect traces for every 2nd HTTP request in Jaeger.

   ``` bash
   istioctl install -y -f - <<EOF
   apiVersion: install.istio.io/v1alpha1
   kind: IstioOperator
   spec:
     meshConfig:
       enableTracing: true
       defaultConfig:
         tracing:
           sampling: 50
   EOF
   ```

   > **Note**
   >
   > Do not use this in production: tracing has substantial resource requirements that may easily overwhelm your cluster.

4. Enable Istio in the `default` namespace. This amounts to switching Istio *sidecar injection* on, which means that Istio will automatically process the traffic of each pod in the `default` namespace to enforce the traffic management policies configured in Istio.

   ```shell
   kubectl label namespace default istio-injection=enabled --overwrite
   ```

   > **Note**
   >
   > The above will affect only newly created pods in the namespace but *does not* restart any workload already running. You have to manually restart your Deployments and StatefulSets to enable Istio to control them.

To actually use Istio on a real workload, we will of course use our handy SplitDim app. Deploy SplitDim with the key-value store backend; if it already runs, make sure to restart it (as well as the key-value store) in order for Istio to take effect. At the end your pod listing should show something like the below:

```shell
kubectl get pod 
NAME                        READY   STATUS    RESTARTS   AGE
kvstore-0                   2/2     Running   0          84s
splitdim-58c48cf87b-wgdfp   2/2     Running   0          89s
```

What is important here is the readiness status: for instance the `READY` column output `2/2` for the `splitdim` pod shows that the pod is running *two* containers, one is of course `splitdim` itself and the other is a *sidecar proxy* injected by Istio to process the pod's ingress/egress traffic.

> **Note**
>
> You may see different output if you use, e.g., [Istio Ambient Mesh](https://istio.io/latest/docs/ops/ambient/getting-started).

Add some initial data to the app:

```shell
export EXTERNAL_IP=$(kubectl get service splitdim -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
export EXTERNAL_PORT=80
curl http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/reset
curl -H "Content-Type: application/json" --request POST --data '{"sender":"a","receiver":"b","amount":2}' http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/transfer
curl -H "Content-Type: application/json" --request POST --data '{"sender":"b","receiver":"c","amount":1}' http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/transfer
```

## Observability

One of the most useful features of a service mesh is that it provided deep insight into the actual communication patterns between microservices: you can see which service (that is, Kubernetes Service!) exchanges traffic with which other services, the instantaneous HTTP request rate and response time(!), HTTP status codes (so you can spot frequent HTTP failure statuses like 404 or 500), you can even trace client calls through the mesh. The low level metrics are collected by [Prometheus](https://istio.io/latest/docs/tasks/observability/metrics/querying-metrics), which can then be visualized in fancy dashboards using [Grafana](https://istio.io/latest/docs/tasks/observability/metrics/using-istio-dashboard). However, by far the most useful (and visually appealing!) observability tool provided by Istio is [Kiali](https://istio.io/latest/docs/tasks/observability/kiali). 

Instead of going into the details of the monitoring goodies what Kiali provided, it is best if you experience it firsthand. First, generate some background traffic: the below will query the `/api/clear` API endpoint 5 times per second and dump the output to the console.

``` bash
watch -n .2 curl -o /dev/null -s http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/clear
```

Now we are ready to launch Kiali:

``` bash
bin/istioctl dashboard kiali
```

Some help:
- The *Overview* panel gives some overall statistics about the cluster, including the CPU usage of Istio itself, total external traffic of your workloads, etc.
- The *Graphs* provides several visual representations of the microservice graph. Use the dropdowns at the top to select a namespace to monitor, the types of traffic to watch (HTTP should be enough for now), the visualization type (we recommend the *Workload graph* view), and the metrics to be displayed (enable the "95-th percentile response time" and the "traffic rate" metrics, and you may want to remove the "Service nodes" to get a per-pod view of your microservice graph). The right-hand sidebar shows the total traffic characteristics, including the failure rate of each workload. By default, metrics are shown for the last 1 minute and update every 10 secs: you can override this using the dropdowns in the upper right corner. Click to any of the nodes: you should see the details for that pod/service.
- The *Applications*, *Workloads* and *Services* panels give different views of your Kubernetes resources that define your workload. Click into any of the workload items: you should see an overview of the item, traffic aggregates, live inbound traffic charts, and even traces showing the response times during the last couple of seconds.

![Service graph from a healthy cluster.](/99-labs/fig/kiali-healthy.png)

> **Note**
>
> You won't see any meaningful output unless there is actual traffic in the cluster. This is because Istio traces out the service graph passively from live traffic, so if there is no traffic, no output.

## Ingress gateway

## Traffic management

choose "Graph", set namespace to default and choose "Workload graph" and set 

``` bash
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata: { name: kvstore-500 }
spec:
  hosts: [ kvstore ]
  http:
    # requests to the "/api/list" path will return 500 status every 10th time
    - match: [ uri: { exact: "/api/list" } ]
      fault: { abort: { httpStatus: 500, percentage: { value: 10 } } }
      route: [ destination: { host: kvstore } ]
    # default route: everything that is not a "put" ("list" and "get")
    - route: [ destination: { host: kvstore } ]
EOF
```



> ✅ **Check**
> Test your Kubernetes deployment. Some useful commands for testing from the shell:

<!-- Local Variables: -->
<!-- mode: markdown; coding: utf-8 -->
<!-- eval: (auto-fill-mode -1) -->
<!-- visual-line-mode: 1 -->
<!-- markdown-enable-math: t -->
<!-- End: -->
